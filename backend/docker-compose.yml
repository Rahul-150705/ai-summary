version: "3.9"

services:

  # ── PostgreSQL 16 + pgvector ─────────────────────────────────────────────
  # pgvector enables storing and searching vector embeddings (used for RAG Q&A)
  db:
    image: pgvector/pgvector:pg16
    container_name: teaching_assistant_db
    restart: unless-stopped
    environment:
      POSTGRES_USER: postgres
      POSTGRES_PASSWORD: rahul18
      POSTGRES_DB: teaching_assistant
    ports:
      - "5432:5432"
    volumes:
      - pgdata:/var/lib/postgresql/data # all DB data persists here
      - ./init.sql:/docker-entrypoint-initdb.d/init.sql # enables vector extension on first start
    healthcheck:
      test: [ "CMD-SHELL", "pg_isready -U postgres -d teaching_assistant" ]
      interval: 10s
      timeout: 5s
      retries: 5

  # ── Ollama — Local AI Model Runner ───────────────────────────────────────
  # Runs AI models 100% locally — no API key, no cost.
  # Your Spring Boot app talks to it at: http://localhost:11434
  ollama:
    image: ollama/ollama
    container_name: ollama
    restart: unless-stopped
    ports:
      - "11434:11434"
    volumes:
      - ollama_data:/root/.ollama # downloaded models persist here (~2+ GB)
    healthcheck:
      test: [ "CMD-SHELL", "curl -sf http://localhost:11434/api/tags || exit 1" ]
      interval: 15s
      timeout: 10s
      retries: 5
    # ── NVIDIA GPU support (uncomment if you have a GPU) ──────────────────
    # deploy:
    #   resources:
    #     reservations:
    #       devices:
    #         - driver: nvidia
    #           count: 1
    #           capabilities: [gpu]

    # ── Ollama Model Initializer (runs once, then exits) ─────────────────────
    # Pulls llama3.2 and nomic-embed-text into Ollama on first startup.
    # Models are cached in ollama_data volume — not re-downloaded on restart.
  ollama-init:
    image: curlimages/curl:latest
    container_name: ollama_model_init
    depends_on:
      ollama:
        condition: service_healthy
    restart: "no"
    entrypoint: >
      sh -c "
        echo '>>> Pulling llama3.2 (chat model ~2 GB)...' &&
        curl -s -X POST http://ollama:11434/api/pull
          -H 'Content-Type: application/json'
          -d '{\"name\":\"llama3.2\",\"stream\":false}' &&
        echo '>>> Pulling nomic-embed-text (embedding model ~274 MB)...' &&
        curl -s -X POST http://ollama:11434/api/pull
          -H 'Content-Type: application/json'
          -d '{\"name\":\"nomic-embed-text\",\"stream\":false}' &&
        echo '>>> All models ready!'
      "

  # ── Spring Boot Backend ───────────────────────────────────────────────────
  # Built from the local Dockerfile (multi-stage Maven build).
  # Environment variables override application.properties defaults so that
  # the app uses Docker service names (db, ollama) instead of localhost.
  backend:
    build:
      context: .
      dockerfile: Dockerfile
    container_name: teaching_assistant_backend
    restart: unless-stopped
    ports:
      - "8080:8080"
    depends_on:
      db:
        condition: service_healthy # wait for PG to be fully ready
      ollama:
        condition: service_healthy # wait for Ollama to be up
    volumes:
      # ── Application Logs ────────────────────────────────────────────────
      # Persists Spring Boot logs outside the container so you can read them
      # even after the container restarts or crashes.
      - backend_logs:/app/logs

      # ── Uploaded PDFs ────────────────────────────────────────────────────
      # If your app ever saves uploaded PDFs to disk (e.g. for re-processing),
      # they survive container restarts here.
      - uploaded_pdfs:/app/uploads
    environment:
      # ── Database ──────────────────────────────────────────────────────────
      DB_URL: jdbc:postgresql://db:5432/teaching_assistant
      DB_USERNAME: postgres
      DB_PASSWORD: rahul18

      # ── LLM Provider (custom HTTP client) ────────────────────────────────
      LLM_PROVIDER: ollama
      OLLAMA_API_URL: http://ollama:11434/api/generate

      # ── Spring AI (RAG embeddings + chat via local Ollama) ────────────────
      SPRING_AI_OLLAMA_BASE_URL: http://ollama:11434

      # ── API Keys (loaded from .env file) ─────────────────────────────────
      OPENAI_API_KEY: ${OPENAI_API_KEY:-not-set}
      JWT_SECRET: ${JWT_SECRET:-3f8e2a1b9c7d4e6f0a5b2c8d3e9f1a4b7c0d5e2f8a3b6c9d2e5f0a1b4c7d8e9}

      # ── Log file location (Spring Boot writes here inside the container) ──
      LOGGING_FILE_NAME: /app/logs/teaching-assistant.log

      # ── JVM Memory ────────────────────────────────────────────────────────
      JAVA_OPTS: "-Xmx512m -Xms256m"
    healthcheck:
      test: [ "CMD-SHELL", "curl -sf http://localhost:8080/api/lecture/health || exit 1" ]
      interval: 30s
      timeout: 10s
      retries: 5
      start_period: 60s # give Spring Boot enough time to start

# ── Named Volumes ─────────────────────────────────────────────────────────────
# All data lives here — NOT inside containers.
# Survives: container restarts, image rebuilds, docker compose down
# Wiped only by: docker compose down -v  (intentional full reset)
volumes:
  pgdata:
    driver: local
    # PostgreSQL database files (tables, indexes, your lecture data, vectors)

  ollama_data:
    driver: local
    # Ollama AI model files (llama3.2 ~2GB, nomic-embed-text ~274MB)
    # Takes time to download once — then instantly available on restart

  backend_logs:
    driver: local
    # Spring Boot application logs
    # Read them with: docker compose exec backend cat /app/logs/teaching-assistant.log

  uploaded_pdfs:
    driver: local
    # Uploaded PDF files (if your app stores them locally)
    # Currently PDFs are processed in-memory, but this is ready for future use
